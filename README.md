#  Creating Data Lakes Using Apache Spark

### 1. Overview and purpose of the Project:

> In this project, we need to create Data Lakes using Apache Spark and ETL data pipeline using Pyspark and Spark sql to load data from flat files(JSON), which are present at Amazon S3, in form of Data lakes on Amazon S3. The data is currently lying in form of JSON files. The JSON files contain factual data about songs and user behavioural data of the song playing App(Sparkify). However, data is not of much use in form of JSON file format for analysis point of view. Hence, task is to load the data into Cloud based Data Lakes, so that analytics team can easily acess it and use it for analysis purpose.

### 2. File Structure and Database Design: 

> The data is present in two JSON files. First one, song dataset, contains metadata about songs and artists of the songs. Other one, log dataset, is basically a log file contains user's logs generated by event simulator.
1. Song dataset contains following columns:['artist_id', 'artist_latitude', 'artist_location', 'artist_longitude', 'artist_name', 'duration', 'num_songs', 'song_id', 'title', 'year']
2. Log dataset contains following columns: ['artist', 'auth', 'firstName', 'gender', 'itemInSession', 'lastName','length', 'level', 'location', 'method', 'page', 'registration','sessionId', 'song', 'status', 'ts', 'userAgent', 'userId']

>Based on the available data in the above two files, for database design star schema would be appropriate seeing the simplicity and its wide use in data warehouse design. Using start schema we can design one fact table and four dimension tables as below: 

>##### Fact Table: 
1. songplays (songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)

>##### Dimesion Tables:
2. users (user_id, first_name, last_name, gender, level)
3. songs (song_id, title, artist_id, year, duration)
4. artists (artist_id, name, location, latitude, longitude)
5. time (start_time, hour, day, week, month, year, weekday)

### 3. Python Scripts used and steps to Run them:

>Followings are the files used in this project:  
1. etl.py: This is a standalone program file. This file contains implementaion of etl pipeline that includes reading JSON files from S3 buckets then processing it, extracting data for the dimension tables and writing it back to the S3 buckets. 
 

>##### Steps to run python scripts in terminal: 
1. Run the script 'etl.py' using command- 'python etl.py'.
